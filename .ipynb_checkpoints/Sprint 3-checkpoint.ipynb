{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# van eerder: alle niet belangrijke dingen wegfilteren\n",
    "#data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(preprocessing)\n",
    "#data[['Consumer complaint narrative', 'TARGET']] bevat nog alle woorden etc voordat tfidf etc werd toegepast. -> gebruiken voor tokenizer\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['Consumer complaint narrative'])\n",
    "encoded_docs = tokenizer.texts_to_matrix(data['Consumer complaint narrative'], mode='count')\n",
    "# sparse matrix, element staat 1 in indien woord in dat document voorkwam\n",
    "print(encoded_docs.shape)\n",
    "#[0] is aantal documetnen, [1] is aantal verschillende woorden -> aantal inputnodes voor nn nodig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTargets = [ord(x[-1])-ord('A') for x in data['TARGET']]\n",
    "onehotTargets = keras.utils.to_categorical(numTargets, 6)\n",
    "onehotTargets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(encoded_docs,onehotTargets,test_size = 0.3)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(encoded_docs.shape[1],)))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(onehotTargets.shape[1], activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
